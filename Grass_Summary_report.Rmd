---
title: "Grass summary report"
output:
  pdf_document: default
  html_document:
    df_print: paged
  html_notebook: default
---
Loading data and evaluating Structure
```{r}
data=load("grass_results4Marcos.RData")
str(data)
```
Structure "Maps"
```{r}
str(maps)
```
Structure "grid"
```{r}
str(grid)
```
Boxplot data visualization

```{r}
par(mfrow = c(2, 2))
boxplot(x=maps[,,1], y=maps[,,1], xlab = "LSU Groups", ylab = "gC/m2", main="Harvest")
boxplot(x=maps[,,2], y=maps[,,2], type = "l", xlab = "LSU Groups", ylab = "gC/m2", main="SoilC")
boxplot(x=maps[,,3], y=maps[,,3], xlab = "LSU Groups", ylab = "gC/m2", main="Npp")
par(mfrow = c(1, 1))
```
Mean gC/m2 per group
```{r}
CO2.concentrations = dimnames(maps)$lsu
means <- matrix(ncol=length(maps[1,1,]), nrow=length(CO2.concentrations))
for (col in 1:ncol(means)) {
  for (row in 1:nrow(means)) {
    means[row,col] <- mean(maps[,row,col])
  }
}
rownames(means) <- c(dimnames(maps)$lsu)
colnames(means) <- dimnames(maps)$var
means
par(mfrow = c(2, 2))
plot(means[,1], type = "l", xlab = "LSU Groups", ylab = "gC/m2", main="Harvest")
plot(means[,2], type = "l", xlab = "LSU Groups", ylab = "gC/m2", main="SoilC")
plot(means[,3], type = "l", xlab = "LSU Groups", ylab = "gC/m2", main="Npp")
par(mfrow = c(1, 1))

```
<h1>Regressing Harvest</h1>
Creating and describing Data.frame for Harvest
```{r}
harvest = NULL
for (i in 1:length(dimnames(maps)$lsu)) {
  cols = cbind("gCm2" = as.numeric(maps[,i,1]),"LSU" = as.numeric(dimnames(maps)$lsu[i]))
  harvest = rbind(harvest, cols)
}
df.harvest <- as.data.frame(harvest)

# Summary of new dataframe
length(df.harvest$LSU)
names(df.harvest)
str(df.harvest)
```
Summary for Data.frame Harvest
```{r}
summary(df.harvest)
```
```{r}
gcm2 = df.harvest[df.harvest$gCm2>0,]
subset = subset(df.harvest, gCm2>0, select = c("gCm2", "LSU"))

hist(as.numeric(df.harvest$LSU))
hist(subset$gCm2)

fit1 = lm(gCm2 ~ LSU + I(LSU^2) + I(LSU^3) + I(LSU^4) + I(LSU^5), data=df.harvest)
summary(fit1)
plot(fit1)

```
 
```{r}
summary(fit1)
```
```{r}
library(keras)
library(ggplot2)

#updating df.harvest to add spatial information

harvest = NULL
for (i in 1:length(dimnames(maps)$lsu)) {
  cols = cbind("gCm2" = as.numeric(maps[,i,1]),"LSU" = as.numeric(dimnames(maps)$lsu[i]), grid)
  harvest = rbind(harvest, cols)
}
df.harvest <- as.data.frame(harvest)

#randomize data in df.harvest

nr<-dim(df.harvest)[1]
df.harvest = df.harvest[sample.int(nr),]

#divide data in training and testing

train_data <- df.harvest[1:667000,2:4]
train_labels <- df.harvest[1:667000,1]

test_data <- df.harvest[667000:741620,2:4]
test_labels <- df.harvest[667000:741620,1]

train_data <- as.matrix(train_data)
train_labels <- as.matrix(train_labels)
test_data <- as.matrix(test_data)
test_labels <- as.matrix(test_labels)

#hypermparameters
epochs <- 10
batch_size <- 100
optimizer <- optimizer_adam()
loss <- "mse"
normalized = TRUE

#scaling

if (normalized) {
train_data <- scale(train_data)
col_means_train <- attr(train_data, "scaled:center")
col_stddevs_train <- attr(train_data, "scaled:scale")
test_data <- scale(test_data, center = col_means_train, scale = col_stddevs_train)
print("Normalized")
}

#building the model


build_model <- function() {
  
  model <- keras_model_sequential() %>%
    layer_dense(units = 500, activation = "relu", input_shape = dim(train_data)[2]) %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(units = 450, activation = "relu") %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(units = 400, activation = "relu") %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(units = 350, activation = "relu") %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(units = 300, activation = "relu") %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(units = 250, activation = "relu") %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(units = 200, activation = "relu") %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(units = 150, activation = "relu") %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(units = 100, activation = "relu") %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(units = 50, activation = "relu") %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(units = 1)

  
  model %>% compile(
    loss = loss,
    optimizer = optimizer,
    metrics = list("mean_absolute_error")
  )
  
  model
}

model <- build_model()
model %>% summary()


# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)

model <- build_model()
history <- model %>% fit(
  train_data,
  train_labels,
  batch_size = batch_size,
  epochs = epochs,
  validation_split = 0.2,
  verbose = 1,
  callbacks = list(early_stop)
)

model %>% save_model_hdf5("my_model.h5")
model %>% save_model_weights_hdf5("my_model_weights.h5")

#evaluating results
plot(history, metrics = "mean_absolute_error", smooth = FALSE)
paste0("Mean absolute error on test set: ", sprintf("%.2f", mae))

  #variable initiated outside the block
  #run=0
  runs = runs+1

test_predictions <- model %>% predict(test_data)
test_results = cbind(test_predictions[1:10, 1],test_labels[1:10,1])

hist(test_predictions)
hist(test_labels)

#writing results
x = list("epochs"= epochs, "batch_size: "= batch_size, "optimizer"= as.character(optimizer), "loss"= loss, "Mean absolute error on test set"= mae, "normalized" = normalized, test_results)
write.csv2(x,file = paste0("run",runs,".csv"))

```
 
 